{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('Deep_Learning_PyTorch': conda)",
   "display_name": "Python 3.8.5 64-bit ('Deep_Learning_PyTorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "61442bf9e5bf21fc60cdab8767f30eb494cb7ce16060a61e724f5081f2841202"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Base Regression\n",
    "\n",
    "This part of the project works to apply the Cross Validation method for validation of a model. Here, we will use the results from the analyses made on the preprocessing step, explained on the file \"CarBase_Regression.ipynb\". \n",
    "\n",
    "As we know, the pytorch library doesn't have a Cross Validation method for the training and validation of our model. So, in order to use this technique, we will use the **Skorch** library. The [Skorch library](https://skorch.readthedocs.io/en/stable/index.html) makes it possible for us to use all the power of Deep Learning within the Pytorch library, and the well known and easy to understand, structure from sklearn library. A simple example is the training of a neural network, where with pytorch, we must create the loop for the training step, passing the data by each layer from our NN model. With Skorch, we must create a class where we will build our Neural Network structure, and pass some parameters to Regressor or classifier method from skorch, as the *learning rate*, *optimization algorithm*,  number of epochs to be used on the training, and some other parameters. Some examples of how to use skorch library can be seen [HERE](https://skorch.readthedocs.io/en/stable/user/quickstart.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from  torch import nn, optim\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x254eb063750>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.read_csv('./autos.zip', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           dateCrawled                            name  seller offerType  \\\n",
       "0  2016-03-24 11:52:17                      Golf_3_1.6  privat   Angebot   \n",
       "1  2016-03-24 10:58:45            A5_Sportback_2.7_Tdi  privat   Angebot   \n",
       "2  2016-03-14 12:52:21  Jeep_Grand_Cherokee_\"Overland\"  privat   Angebot   \n",
       "3  2016-03-17 16:54:04              GOLF_4_1_4__3TÜRER  privat   Angebot   \n",
       "4  2016-03-31 17:25:20  Skoda_Fabia_1.4_TDI_PD_Classic  privat   Angebot   \n",
       "\n",
       "   price abtest vehicleType  yearOfRegistration    gearbox  powerPS  model  \\\n",
       "0    480   test         NaN                1993    manuell        0   golf   \n",
       "1  18300   test       coupe                2011    manuell      190    NaN   \n",
       "2   9800   test         suv                2004  automatik      163  grand   \n",
       "3   1500   test  kleinwagen                2001    manuell       75   golf   \n",
       "4   3600   test  kleinwagen                2008    manuell       69  fabia   \n",
       "\n",
       "   kilometer  monthOfRegistration fuelType       brand notRepairedDamage  \\\n",
       "0     150000                    0   benzin  volkswagen               NaN   \n",
       "1     125000                    5   diesel        audi                ja   \n",
       "2     125000                    8   diesel        jeep               NaN   \n",
       "3     150000                    6   benzin  volkswagen              nein   \n",
       "4      90000                    7   diesel       skoda              nein   \n",
       "\n",
       "           dateCreated  nrOfPictures  postalCode             lastSeen  \n",
       "0  2016-03-24 00:00:00             0       70435  2016-04-07 03:16:57  \n",
       "1  2016-03-24 00:00:00             0       66954  2016-04-07 01:46:50  \n",
       "2  2016-03-14 00:00:00             0       90480  2016-04-05 12:47:46  \n",
       "3  2016-03-17 00:00:00             0       91074  2016-03-17 17:40:17  \n",
       "4  2016-03-31 00:00:00             0       60437  2016-04-06 10:17:21  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dateCrawled</th>\n      <th>name</th>\n      <th>seller</th>\n      <th>offerType</th>\n      <th>price</th>\n      <th>abtest</th>\n      <th>vehicleType</th>\n      <th>yearOfRegistration</th>\n      <th>gearbox</th>\n      <th>powerPS</th>\n      <th>model</th>\n      <th>kilometer</th>\n      <th>monthOfRegistration</th>\n      <th>fuelType</th>\n      <th>brand</th>\n      <th>notRepairedDamage</th>\n      <th>dateCreated</th>\n      <th>nrOfPictures</th>\n      <th>postalCode</th>\n      <th>lastSeen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-03-24 11:52:17</td>\n      <td>Golf_3_1.6</td>\n      <td>privat</td>\n      <td>Angebot</td>\n      <td>480</td>\n      <td>test</td>\n      <td>NaN</td>\n      <td>1993</td>\n      <td>manuell</td>\n      <td>0</td>\n      <td>golf</td>\n      <td>150000</td>\n      <td>0</td>\n      <td>benzin</td>\n      <td>volkswagen</td>\n      <td>NaN</td>\n      <td>2016-03-24 00:00:00</td>\n      <td>0</td>\n      <td>70435</td>\n      <td>2016-04-07 03:16:57</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016-03-24 10:58:45</td>\n      <td>A5_Sportback_2.7_Tdi</td>\n      <td>privat</td>\n      <td>Angebot</td>\n      <td>18300</td>\n      <td>test</td>\n      <td>coupe</td>\n      <td>2011</td>\n      <td>manuell</td>\n      <td>190</td>\n      <td>NaN</td>\n      <td>125000</td>\n      <td>5</td>\n      <td>diesel</td>\n      <td>audi</td>\n      <td>ja</td>\n      <td>2016-03-24 00:00:00</td>\n      <td>0</td>\n      <td>66954</td>\n      <td>2016-04-07 01:46:50</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-03-14 12:52:21</td>\n      <td>Jeep_Grand_Cherokee_\"Overland\"</td>\n      <td>privat</td>\n      <td>Angebot</td>\n      <td>9800</td>\n      <td>test</td>\n      <td>suv</td>\n      <td>2004</td>\n      <td>automatik</td>\n      <td>163</td>\n      <td>grand</td>\n      <td>125000</td>\n      <td>8</td>\n      <td>diesel</td>\n      <td>jeep</td>\n      <td>NaN</td>\n      <td>2016-03-14 00:00:00</td>\n      <td>0</td>\n      <td>90480</td>\n      <td>2016-04-05 12:47:46</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2016-03-17 16:54:04</td>\n      <td>GOLF_4_1_4__3TÜRER</td>\n      <td>privat</td>\n      <td>Angebot</td>\n      <td>1500</td>\n      <td>test</td>\n      <td>kleinwagen</td>\n      <td>2001</td>\n      <td>manuell</td>\n      <td>75</td>\n      <td>golf</td>\n      <td>150000</td>\n      <td>6</td>\n      <td>benzin</td>\n      <td>volkswagen</td>\n      <td>nein</td>\n      <td>2016-03-17 00:00:00</td>\n      <td>0</td>\n      <td>91074</td>\n      <td>2016-03-17 17:40:17</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2016-03-31 17:25:20</td>\n      <td>Skoda_Fabia_1.4_TDI_PD_Classic</td>\n      <td>privat</td>\n      <td>Angebot</td>\n      <td>3600</td>\n      <td>test</td>\n      <td>kleinwagen</td>\n      <td>2008</td>\n      <td>manuell</td>\n      <td>69</td>\n      <td>fabia</td>\n      <td>90000</td>\n      <td>7</td>\n      <td>diesel</td>\n      <td>skoda</td>\n      <td>nein</td>\n      <td>2016-03-31 00:00:00</td>\n      <td>0</td>\n      <td>60437</td>\n      <td>2016-04-06 10:17:21</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features dropped based on their relevance on the project\n",
    "base = base.drop('dateCrawled', axis=1)\n",
    "base = base.drop('dateCreated', axis=1)\n",
    "base = base.drop('nrOfPictures', axis=1)\n",
    "base = base.drop('postalCode', axis=1)\n",
    "base = base.drop('lastSeen', axis=1)\n",
    "\n",
    "# Features dropped based on the analysis made on \"CarBase_Regression.ipynb\" file;\n",
    "base = base.drop('name', axis=1)\n",
    "base = base.drop('seller', axis=1)\n",
    "base = base.drop('offerType', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(371528, 15)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "base.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threating Outliers from price feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = base['price'].quantile(0.25)\n",
    "Q3 = base['price'].quantile(0.75)\n",
    "\n",
    "IQR = Q3-Q1\n",
    "\n",
    "base_ = base[~((base['price'] < (Q1-1.5*IQR)) | (base['price'] > (Q3 + 1.5*IQR)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of observations dropped from the dataset:\t 28108 (7.57%)\n"
     ]
    }
   ],
   "source": [
    "print('number of observations dropped from the dataset:\\t %d (%.2f%%)' % (base.shape[0] - base_.shape[0], (base.shape[0] - base_.shape[0])/base.shape[0]*100))"
   ]
  },
  {
   "source": [
    "Replacing null values on each column (Values to be used instead null, are based on the analysis of the file \"CarBase_Regression.ipynb\"):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'vehicleType': 'limousine', 'gearbox': 'manuell', 'model': 'golf', 'fuelType': 'benzin', 'notRepairedDamage': 'nein'}\n",
    "\n",
    "base_ = base_.fillna(value=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = base_.iloc[:, 1:].values\n",
    "classes = base_.iloc[:, 0].values.reshape(-1, 1)"
   ]
  },
  {
   "source": [
    "Treating Categorical Features with One Hot Encoder technique:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "oneHotEncoder = ColumnTransformer(transformers=[('OneHot', OneHotEncoder(), [0,1,3,5,8,9,10])], remainder='passthrough')\n",
    "\n",
    "features = oneHotEncoder.fit_transform(features).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.astype('float32')\n",
    "classes = classes.astype('float32')"
   ]
  },
  {
   "source": [
    "## Creating the Neural Network Model:\n",
    "\n",
    "Now that we have modeled our data on \"Preprocessing data\" fase, we will create our neural network model so that we can predict the price of a vehicle based on some parameters given to the model.\n",
    "\n",
    "As is explained on skorch documentation, the neural network model must be created within a class, where it will be passed as a parameter to the regressor method used by skorch. \n",
    "\n",
    "So, first, we must create our class:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Structure: 315 -> 158 -> 158 -> 1\n",
    "# Hidden Layer:    (Input + Output)/2 = 158\n",
    "\n",
    "class neuralNetTrain(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense0 = nn.Linear(315, 158)\n",
    "        self.dense1 = nn.Linear(158, 158)\n",
    "        self.Output = nn.Linear(158, 1)\n",
    "        self.activation0 = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.Output(X)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "source": [
    "Now we have created our Neural Network class with it's structure, we must call our Neural Network Regressor, passing some parametgers, as the NN structure to be optimized, the optimization algotithm, number of epochs, and so on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnreg = NeuralNetRegressor(\n",
    "    module=neuralNetTrain,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    optimizer=torch.optim.Adam, \n",
    "    batch_size=300, \n",
    "    max_epochs=100,\n",
    "    device='cuda',\n",
    "    train_split = False)"
   ]
  },
  {
   "source": [
    "Now we have our Neural Network Regressor created, we may call the Cross Validation method from sklearn and train the our model based on this technique."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m2498.4433\u001b[0m  9.6925\n",
      "      2     \u001b[36m2137.1967\u001b[0m  8.8871\n",
      "      3     \u001b[36m2059.0723\u001b[0m  8.5646\n",
      "      4     \u001b[36m2016.5721\u001b[0m  8.5125\n",
      "      5     \u001b[36m1989.0098\u001b[0m  8.5587\n",
      "      6     \u001b[36m1945.3866\u001b[0m  8.8191\n",
      "      7     \u001b[36m1942.6489\u001b[0m  8.6033\n",
      "      8     \u001b[36m1921.8457\u001b[0m  8.5985\n",
      "      9     \u001b[36m1898.5018\u001b[0m  11.5820\n",
      "     10     \u001b[36m1883.5594\u001b[0m  12.3071\n",
      "     11     \u001b[36m1863.4250\u001b[0m  8.6000\n",
      "     12     \u001b[36m1843.5910\u001b[0m  8.4911\n",
      "     13     \u001b[36m1826.1203\u001b[0m  8.7392\n",
      "     14     \u001b[36m1817.9483\u001b[0m  8.5344\n",
      "     15     \u001b[36m1799.3464\u001b[0m  8.6623\n",
      "     16     \u001b[36m1778.0419\u001b[0m  8.3195\n",
      "     17     1784.6129  8.5245\n",
      "     18     \u001b[36m1757.6450\u001b[0m  8.5236\n",
      "     19     \u001b[36m1753.2489\u001b[0m  9.2406\n",
      "     20     1762.1706  8.6955\n",
      "     21     \u001b[36m1750.7953\u001b[0m  8.5539\n",
      "     22     \u001b[36m1743.8013\u001b[0m  8.5469\n",
      "     23     1745.1796  8.4181\n",
      "     24     \u001b[36m1719.9659\u001b[0m  8.5192\n",
      "     25     1744.8844  8.5527\n",
      "     26     1736.9957  8.5491\n",
      "     27     1724.6751  8.3174\n",
      "     28     1728.2754  8.9817\n",
      "     29     1721.6908  8.8015\n",
      "     30     1721.6672  10.2071\n",
      "     31     1723.5976  8.9170\n",
      "     32     1729.5553  8.7892\n",
      "     33     1725.6173  15.0734\n",
      "     34     \u001b[36m1714.2819\u001b[0m  8.3676\n",
      "     35     1720.5619  8.5041\n",
      "     36     \u001b[36m1710.4038\u001b[0m  9.0454\n",
      "     37     \u001b[36m1707.0261\u001b[0m  9.0594\n",
      "     38     \u001b[36m1706.8859\u001b[0m  8.5261\n",
      "     39     1713.0521  8.3425\n",
      "     40     1708.6660  8.7611\n",
      "     41     \u001b[36m1697.8245\u001b[0m  8.6200\n",
      "     42     \u001b[36m1697.4094\u001b[0m  8.5474\n",
      "     43     1699.0019  8.3138\n",
      "     44     1702.4060  8.6240\n",
      "     45     1697.9754  8.7463\n",
      "     46     \u001b[36m1693.0103\u001b[0m  8.7679\n",
      "     47     1701.7993  10.5942\n",
      "     48     1693.2086  10.6490\n",
      "     49     1693.3051  9.2838\n",
      "     50     \u001b[36m1690.5136\u001b[0m  9.4939\n",
      "     51     \u001b[36m1686.3241\u001b[0m  8.5525\n",
      "     52     1690.8660  8.3998\n",
      "     53     \u001b[36m1685.7091\u001b[0m  8.5484\n",
      "     54     1686.6394  8.7718\n",
      "     55     1691.8843  8.5272\n",
      "     56     \u001b[36m1684.4157\u001b[0m  8.3380\n",
      "     57     \u001b[36m1682.6347\u001b[0m  8.4887\n",
      "     58     1686.5192  8.5496\n",
      "     59     1688.3212  8.5845\n",
      "     60     \u001b[36m1674.7833\u001b[0m  8.3206\n",
      "     61     1698.7674  8.3243\n",
      "     62     1686.5661  8.5671\n",
      "     63     \u001b[36m1672.3974\u001b[0m  8.5661\n",
      "     64     1683.3520  8.4989\n",
      "     65     1686.4186  8.3293\n",
      "     66     1680.0588  8.5324\n",
      "     67     1681.1685  8.5021\n",
      "     68     1679.6150  8.9742\n",
      "     69     1682.0671  8.3820\n",
      "     70     1678.4902  8.3500\n",
      "     71     1678.1827  8.5132\n",
      "     72     \u001b[36m1668.4196\u001b[0m  8.5497\n",
      "     73     1673.3158  8.6962\n",
      "     74     \u001b[36m1668.1669\u001b[0m  8.3020\n",
      "     75     1669.3320  10.0759\n",
      "     76     \u001b[36m1666.6884\u001b[0m  9.4490\n",
      "     77     \u001b[36m1665.5791\u001b[0m  8.4881\n",
      "     78     1675.4319  9.6279\n",
      "     79     1671.8430  8.5551\n",
      "     80     \u001b[36m1663.1170\u001b[0m  8.5069\n",
      "     81     1669.4541  8.5069\n",
      "     82     1666.7284  8.3206\n",
      "     83     1673.2273  8.5940\n",
      "     84     1670.1862  8.8693\n",
      "     85     1667.3283  9.9089\n",
      "     86     1663.6086  12.1675\n",
      "     87     1665.4835  8.7084\n",
      "     88     \u001b[36m1662.2601\u001b[0m  8.3573\n",
      "     89     1665.6558  8.5808\n",
      "     90     \u001b[36m1656.5610\u001b[0m  8.8494\n",
      "     91     1662.5946  8.5174\n",
      "     92     1659.2636  8.5145\n",
      "     93     1661.0135  8.3422\n",
      "     94     1660.4358  8.5472\n",
      "     95     1659.4616  8.5407\n",
      "     96     \u001b[36m1654.6931\u001b[0m  8.5153\n",
      "     97     1659.6588  8.4137\n",
      "     98     1661.1402  8.3356\n",
      "     99     \u001b[36m1652.5244\u001b[0m  8.0701\n",
      "    100     1666.7201  7.9241\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m2466.2778\u001b[0m  7.6614\n",
      "      2     \u001b[36m2117.0865\u001b[0m  7.5162\n",
      "      3     \u001b[36m2046.9342\u001b[0m  7.6707\n",
      "      4     \u001b[36m2015.5413\u001b[0m  7.5360\n",
      "      5     \u001b[36m2002.5431\u001b[0m  7.6933\n",
      "      6     \u001b[36m1968.3082\u001b[0m  7.5278\n",
      "      7     \u001b[36m1948.9375\u001b[0m  7.6850\n",
      "      8     \u001b[36m1918.4453\u001b[0m  7.5024\n",
      "      9     \u001b[36m1904.6579\u001b[0m  7.6790\n",
      "     10     \u001b[36m1894.5334\u001b[0m  7.5372\n",
      "     11     \u001b[36m1841.6490\u001b[0m  7.7213\n",
      "     12     1854.5450  7.5732\n",
      "     13     \u001b[36m1821.8552\u001b[0m  7.7763\n",
      "     14     \u001b[36m1821.3632\u001b[0m  7.7226\n",
      "     15     \u001b[36m1791.3846\u001b[0m  7.4964\n",
      "     16     \u001b[36m1784.7245\u001b[0m  7.6895\n",
      "     17     1790.7977  7.5098\n",
      "     18     \u001b[36m1775.3634\u001b[0m  7.7498\n",
      "     19     1775.8875  7.5279\n",
      "     20     \u001b[36m1759.2505\u001b[0m  7.7454\n",
      "     21     1759.2941  7.6277\n",
      "     22     \u001b[36m1755.0903\u001b[0m  7.7049\n",
      "     23     \u001b[36m1754.7661\u001b[0m  7.5106\n",
      "     24     \u001b[36m1750.8168\u001b[0m  7.7287\n",
      "     25     \u001b[36m1740.1660\u001b[0m  7.5758\n",
      "     26     1744.0952  8.1339\n",
      "     27     \u001b[36m1729.4679\u001b[0m  7.8158\n",
      "     28     1731.1380  7.5279\n",
      "     29     \u001b[36m1719.4717\u001b[0m  7.6791\n",
      "     30     1729.8776  7.5127\n",
      "     31     1730.0753  7.7116\n",
      "     32     1721.8548  7.5283\n",
      "     33     1723.5788  7.9799\n",
      "     34     1723.2397  7.7532\n",
      "     35     1729.2367  7.8229\n",
      "     36     \u001b[36m1709.5510\u001b[0m  7.7122\n",
      "     37     1715.6160  7.5327\n",
      "     38     1712.1826  7.6575\n",
      "     39     \u001b[36m1707.1711\u001b[0m  7.8636\n",
      "     40     \u001b[36m1703.1925\u001b[0m  7.5021\n",
      "     41     1711.2867  7.6987\n",
      "     42     \u001b[36m1698.7520\u001b[0m  7.5186\n",
      "     43     1707.1877  7.6913\n",
      "     44     1700.1077  7.5190\n",
      "     45     \u001b[36m1695.7180\u001b[0m  7.8614\n",
      "     46     \u001b[36m1692.2403\u001b[0m  7.9645\n",
      "     47     1701.5357  8.8543\n",
      "     48     \u001b[36m1690.1744\u001b[0m  8.5480\n",
      "     49     1695.3666  7.6962\n",
      "     50     1695.3487  7.5303\n",
      "     51     1714.3450  7.6683\n",
      "     52     \u001b[36m1687.8560\u001b[0m  7.7250\n",
      "     53     \u001b[36m1687.0762\u001b[0m  7.5230\n",
      "     54     \u001b[36m1685.7212\u001b[0m  7.7066\n",
      "     55     1694.1065  7.5224\n",
      "     56     1686.0422  7.6915\n",
      "     57     1695.2468  7.5159\n",
      "     58     1687.6212  7.7286\n",
      "     59     1687.8954  8.4310\n",
      "     60     1686.8573  7.8899\n",
      "     61     1688.1528  7.8879\n",
      "     62     \u001b[36m1679.3419\u001b[0m  7.8919\n",
      "     63     1681.3059  7.5226\n",
      "     64     1694.4721  7.7399\n",
      "     65     \u001b[36m1676.4479\u001b[0m  7.5663\n",
      "     66     1681.2828  7.5702\n",
      "     67     \u001b[36m1675.2223\u001b[0m  7.7305\n",
      "     68     1684.5953  7.4780\n",
      "     69     1678.6880  7.6725\n",
      "     70     1687.9882  7.5060\n",
      "     71     1677.6203  7.6816\n",
      "     72     \u001b[36m1673.0427\u001b[0m  7.6268\n",
      "     73     1681.3984  7.5004\n",
      "     74     \u001b[36m1671.3929\u001b[0m  7.7455\n",
      "     75     1678.1261  7.5276\n",
      "     76     1678.0736  7.6716\n",
      "     77     1677.3274  7.5649\n",
      "     78     1674.7519  7.5812\n",
      "     79     \u001b[36m1668.1944\u001b[0m  7.6762\n",
      "     80     \u001b[36m1665.7274\u001b[0m  7.5852\n",
      "     81     1668.0103  8.5387\n",
      "     82     1671.1841  7.9218\n",
      "     83     1673.7407  7.8031\n",
      "     84     1669.8578  7.4750\n",
      "     85     \u001b[36m1665.4554\u001b[0m  7.6718\n",
      "     86     1673.8102  7.5735\n",
      "     87     1673.5075  7.7053\n",
      "     88     1665.9645  7.6075\n",
      "     89     1669.2003  7.5302\n",
      "     90     1671.7846  7.6839\n",
      "     91     1668.9127  7.5232\n",
      "     92     \u001b[36m1659.3495\u001b[0m  7.6555\n",
      "     93     1669.5491  7.7014\n",
      "     94     1669.1932  7.5286\n",
      "     95     1671.3425  7.6778\n",
      "     96     1668.2531  7.5662\n",
      "     97     1670.0226  7.6025\n",
      "     98     1664.7490  7.6720\n",
      "     99     1666.7830  7.5035\n",
      "    100     1664.0030  7.6793\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m2444.0766\u001b[0m  7.6749\n",
      "      2     \u001b[36m2145.0454\u001b[0m  7.5535\n",
      "      3     \u001b[36m2078.5799\u001b[0m  7.6947\n",
      "      4     \u001b[36m2021.4994\u001b[0m  7.5569\n",
      "      5     \u001b[36m2004.4741\u001b[0m  7.6952\n",
      "      6     \u001b[36m1965.6438\u001b[0m  7.5393\n",
      "      7     \u001b[36m1953.1239\u001b[0m  7.6213\n",
      "      8     \u001b[36m1930.3670\u001b[0m  7.7299\n",
      "      9     \u001b[36m1897.7514\u001b[0m  7.5536\n",
      "     10     \u001b[36m1893.1509\u001b[0m  7.6633\n",
      "     11     \u001b[36m1871.5966\u001b[0m  7.5041\n",
      "     12     \u001b[36m1862.2765\u001b[0m  7.7016\n",
      "     13     \u001b[36m1824.5016\u001b[0m  7.5081\n",
      "     14     1825.3501  7.7775\n",
      "     15     \u001b[36m1805.6527\u001b[0m  7.5393\n",
      "     16     \u001b[36m1793.2117\u001b[0m  7.6486\n",
      "     17     1797.8670  7.4983\n",
      "     18     \u001b[36m1772.4963\u001b[0m  7.7022\n",
      "     19     1775.9221  7.5512\n",
      "     20     \u001b[36m1767.8948\u001b[0m  7.7426\n",
      "     21     \u001b[36m1762.5520\u001b[0m  8.4509\n",
      "     22     \u001b[36m1750.5501\u001b[0m  12.3231\n",
      "     23     1762.4684  8.3829\n",
      "     24     \u001b[36m1748.0292\u001b[0m  7.5487\n",
      "     25     1750.3475  7.7175\n",
      "     26     \u001b[36m1738.0567\u001b[0m  7.8724\n",
      "     27     1743.0079  7.5498\n",
      "     28     \u001b[36m1735.6807\u001b[0m  7.6892\n",
      "     29     1744.5397  7.4945\n",
      "     30     \u001b[36m1727.6759\u001b[0m  7.6726\n",
      "     31     1731.7325  7.5601\n",
      "     32     1733.2925  7.6688\n",
      "     33     \u001b[36m1725.9086\u001b[0m  7.5481\n",
      "     34     1726.6264  8.4031\n",
      "     35     \u001b[36m1714.2238\u001b[0m  7.7030\n",
      "     36     1719.9425  7.5595\n",
      "     37     1714.7446  7.6599\n",
      "     38     1720.9442  7.5735\n",
      "     39     \u001b[36m1704.8801\u001b[0m  7.7014\n",
      "     40     1713.8253  7.5423\n",
      "     41     1713.3232  7.5818\n",
      "     42     1709.6466  7.7056\n",
      "     43     \u001b[36m1702.5885\u001b[0m  7.4998\n",
      "     44     \u001b[36m1701.9632\u001b[0m  7.6865\n",
      "     45     1702.8901  7.4912\n",
      "     46     \u001b[36m1700.6774\u001b[0m  8.2048\n",
      "     47     \u001b[36m1697.1506\u001b[0m  7.6109\n",
      "     48     1698.6369  7.4876\n",
      "     49     \u001b[36m1696.6903\u001b[0m  7.6801\n",
      "     50     \u001b[36m1693.1905\u001b[0m  7.5335\n",
      "     51     \u001b[36m1689.7282\u001b[0m  7.6845\n",
      "     52     1693.1830  7.5077\n",
      "     53     1690.8761  7.6925\n",
      "     54     \u001b[36m1688.7605\u001b[0m  7.5770\n",
      "     55     1689.0476  7.5161\n",
      "     56     1694.0781  7.7347\n",
      "     57     \u001b[36m1684.9321\u001b[0m  7.5324\n",
      "     58     1686.6855  7.6765\n",
      "     59     1686.2526  7.5581\n",
      "     60     1687.5638  7.7259\n",
      "     61     \u001b[36m1682.8550\u001b[0m  7.6956\n",
      "     62     \u001b[36m1680.4878\u001b[0m  7.7861\n",
      "     63     1681.3705  7.9737\n",
      "     64     \u001b[36m1678.6801\u001b[0m  7.8448\n",
      "     65     1680.7328  7.7131\n",
      "     66     1683.6858  7.8674\n",
      "     67     \u001b[36m1675.2172\u001b[0m  7.5336\n",
      "     68     1687.0587  7.7396\n",
      "     69     \u001b[36m1672.6358\u001b[0m  7.6595\n",
      "     70     1675.3478  7.5425\n",
      "     71     1673.3707  7.7367\n",
      "     72     1684.8482  7.5734\n",
      "     73     \u001b[36m1672.1632\u001b[0m  7.7287\n",
      "     74     \u001b[36m1667.5054\u001b[0m  7.5340\n",
      "     75     1677.7649  7.5068\n",
      "     76     1673.2749  7.7131\n",
      "     77     1668.5270  7.5603\n",
      "     78     1669.4300  7.6941\n",
      "     79     1677.9082  7.5778\n",
      "     80     1667.9197  7.5419\n",
      "     81     1670.2868  7.6929\n",
      "     82     1669.2199  7.5244\n",
      "     83     1669.0259  7.6840\n",
      "     84     1671.8589  7.6631\n",
      "     85     \u001b[36m1658.2326\u001b[0m  7.5319\n",
      "     86     1666.6304  7.7183\n",
      "     87     1670.1650  7.5521\n",
      "     88     1661.2830  7.7161\n",
      "     89     1659.0982  7.8002\n",
      "     90     1672.5215  7.5160\n",
      "     91     1666.5740  7.7068\n",
      "     92     1661.8152  7.5409\n",
      "     93     1665.1528  7.8156\n",
      "     94     1661.3039  7.7812\n",
      "     95     \u001b[36m1655.6888\u001b[0m  7.5355\n",
      "     96     1666.4037  8.8027\n",
      "     97     1658.3836  8.8057\n",
      "     98     1661.3920  7.6989\n",
      "     99     1664.8298  7.5256\n",
      "    100     1656.1537  7.6772\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m2462.0070\u001b[0m  7.7373\n",
      "      2     \u001b[36m2132.0558\u001b[0m  7.5581\n",
      "      3     \u001b[36m2050.5294\u001b[0m  7.7033\n",
      "      4     \u001b[36m2023.5432\u001b[0m  7.5164\n",
      "      5     \u001b[36m1991.2338\u001b[0m  7.6622\n",
      "      6     \u001b[36m1964.0611\u001b[0m  7.5554\n",
      "      7     \u001b[36m1942.0221\u001b[0m  7.6927\n",
      "      8     \u001b[36m1906.7878\u001b[0m  7.6969\n",
      "      9     \u001b[36m1882.4208\u001b[0m  8.0236\n",
      "     10     \u001b[36m1851.7447\u001b[0m  7.6840\n",
      "     11     \u001b[36m1850.4321\u001b[0m  7.5769\n",
      "     12     \u001b[36m1827.4491\u001b[0m  7.6782\n",
      "     13     \u001b[36m1813.8650\u001b[0m  7.5306\n",
      "     14     1825.2179  7.9010\n",
      "     15     \u001b[36m1784.5228\u001b[0m  7.7912\n",
      "     16     1789.4723  7.8643\n",
      "     17     \u001b[36m1772.5670\u001b[0m  7.8413\n",
      "     18     1777.7856  7.7673\n",
      "     19     \u001b[36m1760.7452\u001b[0m  7.8477\n",
      "     20     \u001b[36m1754.5186\u001b[0m  7.6955\n",
      "     21     \u001b[36m1739.2257\u001b[0m  7.8594\n",
      "     22     1749.6790  7.7170\n",
      "     23     1755.5435  7.7317\n",
      "     24     1751.8230  7.8531\n",
      "     25     \u001b[36m1726.4092\u001b[0m  7.6198\n",
      "     26     1743.6547  7.7239\n",
      "     27     1727.5089  7.5629\n",
      "     28     1733.4174  7.6797\n",
      "     29     1726.6030  7.5672\n",
      "     30     \u001b[36m1724.1837\u001b[0m  7.6999\n",
      "     31     1727.1056  7.5917\n",
      "     32     1730.2311  7.5600\n",
      "     33     \u001b[36m1716.6195\u001b[0m  7.6978\n",
      "     34     1717.8816  7.5668\n",
      "     35     \u001b[36m1714.6475\u001b[0m  7.7544\n",
      "     36     1718.5361  7.5551\n",
      "     37     1717.0877  7.8436\n",
      "     38     1743.2467  7.5691\n",
      "     39     \u001b[36m1710.2583\u001b[0m  7.7820\n",
      "     40     \u001b[36m1705.8939\u001b[0m  7.8297\n",
      "     41     1717.8587  7.5998\n",
      "     42     1714.7592  9.6482\n",
      "     43     1706.4464  9.5824\n",
      "     44     1718.9403  8.4673\n",
      "     45     \u001b[36m1704.8901\u001b[0m  8.1875\n",
      "     46     1710.2192  7.8142\n",
      "     47     1706.6861  8.4465\n",
      "     48     1719.2601  7.9290\n",
      "     49     \u001b[36m1699.8571\u001b[0m  7.5642\n",
      "     50     1703.3470  7.7123\n",
      "     51     1720.2351  7.5531\n",
      "     52     1710.4316  7.7178\n",
      "     53     1703.5607  7.5454\n",
      "     54     \u001b[36m1691.6470\u001b[0m  7.6763\n",
      "     55     1711.7508  7.7056\n",
      "     56     \u001b[36m1684.7187\u001b[0m  7.5790\n",
      "     57     \u001b[36m1684.0325\u001b[0m  7.7391\n",
      "     58     1687.8074  7.5909\n",
      "     59     1705.4411  7.8855\n",
      "     60     1689.8135  7.7011\n",
      "     61     1684.6805  7.5808\n",
      "     62     \u001b[36m1682.9099\u001b[0m  7.7709\n",
      "     63     \u001b[36m1681.8563\u001b[0m  7.5331\n",
      "     64     \u001b[36m1681.1368\u001b[0m  8.0086\n",
      "     65     1691.6754  7.7938\n",
      "     66     1690.3718  7.5885\n",
      "     67     1692.9041  7.6839\n",
      "     68     1694.6171  7.5243\n",
      "     69     \u001b[36m1675.3341\u001b[0m  7.7738\n",
      "     70     1686.2862  7.7062\n",
      "     71     1677.3592  9.0085\n",
      "     72     \u001b[36m1673.7867\u001b[0m  8.3435\n",
      "     73     1678.0497  7.7372\n",
      "     74     \u001b[36m1672.6435\u001b[0m  7.5732\n",
      "     75     \u001b[36m1666.8531\u001b[0m  7.5006\n",
      "     76     1676.1358  7.6945\n",
      "     77     1684.6994  7.5888\n",
      "     78     1671.7093  7.7461\n",
      "     79     1682.2686  7.5923\n",
      "     80     1679.3756  7.7622\n",
      "     81     \u001b[36m1665.9107\u001b[0m  8.0335\n",
      "     82     \u001b[36m1664.4678\u001b[0m  7.7961\n",
      "     83     1674.8082  7.5631\n",
      "     84     1674.3548  8.4921\n",
      "     85     1672.7360  7.6479\n",
      "     86     1675.6693  7.5664\n",
      "     87     1673.0737  7.6962\n",
      "     88     1673.6761  7.5657\n",
      "     89     1669.8508  7.7473\n",
      "     90     1667.6151  7.6984\n",
      "     91     \u001b[36m1660.6848\u001b[0m  7.5460\n",
      "     92     1667.7556  7.7127\n",
      "     93     1675.6592  7.5394\n",
      "     94     1669.2149  7.6907\n",
      "     95     1672.6982  7.7012\n",
      "     96     1668.3392  7.6409\n",
      "     97     1666.8761  7.8197\n",
      "     98     1670.2110  7.7120\n",
      "     99     1667.3345  7.5818\n",
      "    100     1667.2902  7.6916\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1     \u001b[36m2442.3416\u001b[0m  7.6764\n",
      "      2     \u001b[36m2223.3509\u001b[0m  7.5463\n",
      "      3     \u001b[36m2110.5561\u001b[0m  7.7107\n",
      "      4     \u001b[36m2075.6125\u001b[0m  7.5351\n",
      "      5     \u001b[36m2064.0016\u001b[0m  7.6773\n",
      "      6     \u001b[36m2019.8424\u001b[0m  7.5318\n",
      "      7     \u001b[36m1986.4493\u001b[0m  7.6815\n",
      "      8     \u001b[36m1965.5071\u001b[0m  7.5118\n",
      "      9     \u001b[36m1961.9285\u001b[0m  7.7089\n",
      "     10     \u001b[36m1941.7239\u001b[0m  7.6579\n",
      "     11     \u001b[36m1937.5309\u001b[0m  7.5740\n",
      "     12     1945.1553  7.7152\n",
      "     13     \u001b[36m1907.3918\u001b[0m  7.5444\n",
      "     14     \u001b[36m1889.7111\u001b[0m  7.7226\n",
      "     15     1919.4310  7.5106\n",
      "     16     1905.5825  7.7127\n",
      "     17     1901.2836  7.5083\n",
      "     18     1951.3446  7.6933\n",
      "     19     1924.3325  7.5110\n",
      "     20     1910.0399  7.7014\n",
      "     21     1934.2281  7.5259\n",
      "     22     1936.2654  7.7497\n",
      "     23     1929.8897  7.7412\n",
      "     24     1989.4558  7.5709\n",
      "     25     1999.9826  7.6900\n",
      "     26     1944.6437  7.5450\n",
      "     27     1935.6068  7.7105\n",
      "     28     1946.2971  7.5081\n",
      "     29     1923.4190  7.7613\n",
      "     30     1981.9737  7.5474\n",
      "     31     1981.0122  7.7184\n",
      "     32     1975.3056  7.5094\n",
      "     33     2003.7598  7.5885\n",
      "     34     1993.4559  7.7463\n",
      "     35     1963.1844  7.5419\n",
      "     36     2048.2140  7.7139\n",
      "     37     2108.2814  7.5525\n",
      "     38     1943.7531  7.7318\n",
      "     39     1966.9784  7.7310\n",
      "     40     1961.4371  8.1468\n",
      "     41     1945.3995  7.7577\n",
      "     42     2005.1076  7.5305\n",
      "     43     1995.1420  7.7040\n",
      "     44     1983.8730  7.5521\n",
      "     45     2006.8717  7.7965\n",
      "     46     2114.1785  10.7653\n",
      "     47     1972.1878  10.5007\n",
      "     48     2082.0587  7.7616\n",
      "     49     2009.2481  7.6280\n",
      "     50     2040.4339  7.6350\n",
      "     51     2042.1716  7.8776\n",
      "     52     2028.2740  7.5532\n",
      "     53     2088.0837  7.7006\n",
      "     54     1997.6749  7.5250\n",
      "     55     2042.3282  7.6967\n",
      "     56     2091.6762  7.7043\n",
      "     57     2035.2790  7.5174\n",
      "     58     1963.7524  7.7082\n",
      "     59     1992.9392  8.0095\n",
      "     60     1974.4247  7.9109\n",
      "     61     1920.1962  7.6984\n",
      "     62     1994.4405  7.5818\n",
      "     63     2039.2789  7.7221\n",
      "     64     1970.5761  7.5372\n",
      "     65     2042.8237  7.7382\n",
      "     66     2004.4755  7.7527\n",
      "     67     1957.2681  7.7383\n",
      "     68     2092.9599  7.7395\n",
      "     69     2056.5157  7.5372\n",
      "     70     2094.4019  7.7451\n",
      "     71     2024.9668  8.3668\n",
      "     72     2015.7865  7.5271\n",
      "     73     2010.1832  7.6885\n",
      "     74     2112.6409  7.7137\n",
      "     75     2187.5997  7.5731\n",
      "     76     2044.4527  7.7070\n",
      "     77     2032.0595  7.5971\n",
      "     78     1969.6008  7.9418\n",
      "     79     2091.3324  7.8647\n",
      "     80     2075.1644  7.5564\n",
      "     81     2041.4966  7.7569\n",
      "     82     2013.2450  7.5583\n",
      "     83     2078.2456  7.5431\n",
      "     84     2157.8265  7.7059\n",
      "     85     2111.6059  7.5648\n",
      "     86     1946.5039  7.7202\n",
      "     87     2012.8365  7.7410\n",
      "     88     2029.3794  8.7511\n",
      "     89     2016.4909  9.5800\n",
      "     90     2045.6559  8.2352\n",
      "     91     2104.9248  7.6929\n",
      "     92     2153.7337  7.5954\n",
      "     93     1970.1101  7.5304\n",
      "     94     2059.6008  7.7813\n",
      "     95     2068.4098  7.7237\n",
      "     96     2019.5303  8.1835\n",
      "     97     2015.4265  7.7604\n",
      "     98     2078.5696  7.5473\n",
      "     99     2017.8773  7.8414\n",
      "    100     1964.9758  7.7798\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(nnreg, features, classes, cv=5, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = results.mean()\n",
    "std_dev = results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mean score:\t-1758.99980 \nstandard deviation: \t44.63939\n"
     ]
    }
   ],
   "source": [
    "print('mean score:\\t%0.05f \\nstandard deviation: \\t%0.05f' %(mean, std_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}